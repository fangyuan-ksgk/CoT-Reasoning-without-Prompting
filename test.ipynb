{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a3895cd126240ba9b4d618fa8fe00d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from decode import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### It seems that, with AlphaMonarch model, the phenomenom is not obvious on the displayed cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fangyuanyu/Implementation/CoT-decoding/decode.py:129: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  k_prob = torch.nn.functional.softmax(logit[0][logit[0].argsort()[-k:]])\n",
      "/Users/fangyuanyu/Implementation/CoT-decoding/decode.py:63: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  path_prob = torch.nn.functional.softmax(path_prob)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('', tensor(0.0008)), ('There', tensor(0.0048)), ('is', tensor(0.0250)), ('no', tensor(0.9646)), ('certainty', tensor(0.2502)), ('about', tensor(0.0148)), ('the', tensor(0.0022)), ('coin', tensor(0.0003)), ('still', tensor(2.9097e-05)), ('being', tensor(7.7748e-05)), ('heads', tensor(0.0108)), ('up', tensor(0.0010)), ('after', tensor(7.0251e-05)), ('both', tensor(0.0917)), ('flips.</s>', tensor(0.0725))]\n",
      "--------------------\n",
      "[('', tensor(0.0008)), ('Without', tensor(0.0048)), ('specific', tensor(0.0250)), ('information', tensor(0.9646)), ('about', tensor(1.0000)), ('the', tensor(8.1815e-13)), ('outcomes', tensor(1.0710e-11)), ('of', tensor(1.2278e-12)), ('the', tensor(1.1295e-11)), ('flips,', tensor(1.1303e-09)), ('we', tensor(2.5519e-10)), ('cannot', tensor(2.1313e-11)), ('determine', tensor(1.7832e-07)), ('if', tensor(2.4978e-10)), ('the', tensor(1.2873e-10)), ('coin', tensor(3.2241e-10)), ('remains', tensor(7.7641e-11)), ('heads', tensor(5.8527e-10)), ('up', tensor(2.4776e-07)), ('after', tensor(7.3093e-09)), ('both', tensor(8.1569e-11)), ('flips.</s>', tensor(1.0181e-08))]\n",
      "--------------------\n",
      "[('', tensor(0.0008)), ('not', tensor(0.0048)), ('necessarily.', tensor(0.4948)), ('', tensor(0.0016)), ('It', tensor(4.7916e-10)), ('only', tensor(1.0861e-10)), ('remained', tensor(8.9520e-10)), ('heads', tensor(0.9982)), ('up', tensor(4.9277e-12)), ('until', tensor(7.8245e-12)), ('the', tensor(5.5865e-11)), ('moment', tensor(1.4468e-09)), ('Conception', tensor(1.9564e-10)), ('flipped', tensor(1.5392e-10)), ('it.', tensor(7.6181e-07)), ('', tensor(4.2073e-10)), ('The', tensor(3.7410e-06)), ('given', tensor(1.7847e-09)), ('context', tensor(2.9231e-12)), (\"doesn't\", tensor(6.1972e-05)), ('provide', tensor(1.5996e-11)), ('information', tensor(2.1456e-11)), ('about', tensor(1.0804e-06)), ('the', tensor(1.1144e-07)), ('outcome', tensor(4.6924e-11)), ('of', tensor(1.6222e-11)), ('the', tensor(6.2182e-11)), ('second', tensor(1.4178e-10)), ('flip.', tensor(2.6992e-10)), ('', tensor(6.2788e-10)), ('', tensor(1.8348e-10))]\n",
      "--------------------\n",
      "[(':', tensor(0.0009)), ('it', tensor(0.0086)), ('cannot', tensor(0.0250)), ('be', tensor(0.9646)), ('definitively', tensor(1.2028e-09)), ('assumed', tensor(1.0323e-08)), ('without', tensor(4.3028e-06)), ('knowing', tensor(5.6950e-08)), ('the', tensor(1.9835e-05)), ('outcome', tensor(4.2209e-08)), ('of', tensor(1.3564e-06)), ('the', tensor(2.0796e-08)), ('flips', tensor(7.0255e-06)), ('by', tensor(2.3268e-05)), ('Fletcher', tensor(0.0001)), ('and', tensor(7.6556e-09)), ('Conception.</s>', tensor(0.2481))]\n",
      "--------------------\n",
      "[('', tensor(0.0008)), ('Without', tensor(0.0048)), ('specific', tensor(0.0250)), ('information', tensor(0.9646)), ('about', tensor(1.0000)), ('the', tensor(8.1815e-13)), ('outcomes', tensor(1.0710e-11)), ('of', tensor(1.2278e-12)), ('the', tensor(1.1295e-11)), ('flips,', tensor(1.1303e-09)), ('we', tensor(2.5519e-10)), ('cannot', tensor(2.1313e-11)), ('determine', tensor(1.7832e-07)), ('if', tensor(2.4978e-10)), ('the', tensor(1.2873e-10)), ('coin', tensor(3.2241e-10)), ('remains', tensor(7.7641e-11)), ('heads', tensor(5.8527e-10)), ('up', tensor(2.4776e-07)), ('after', tensor(7.3093e-09)), ('both', tensor(8.1569e-11)), ('flips.</s>', tensor(1.0181e-08))]\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "query=\"A coin is heads up. Fletcher flips the coin. Conception flips the coin. Is the coin still heads up?\"\n",
    "template = \"\"\"[INST]{question}[/INST]\n",
    "               \"\"\"\n",
    "\n",
    "# k_response = get_k_path_prob(model, tokenizer, template.format(question=query), k=1)\n",
    "k_response = get_k_path_prob_follow_up(model, tokenizer, template.format(question=query), k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
