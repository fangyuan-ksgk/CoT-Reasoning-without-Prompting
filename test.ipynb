{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d326888e645940abb9418e95b9c50674",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from decode import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### It seems that, with AlphaMonarch model, the phenomenom is not obvious on the displayed cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('5', tensor(0.0957)), ('heads,', tensor(0.2377)), ('4', tensor(0.1666)), ('tails.', tensor(3.5910e-05)), ('', tensor(0.0006)), ('', tensor(0.0008)), ('\"', tensor(0.0829)), ('\"No', tensor(2.3499e-05)), ('\"No,\"', tensor(0.0001)), ('says', tensor(8.2030e-06)), ('Con', tensor(9.3868e-06)), ('Conception', tensor(2.2744e-05)), ('Conception.', tensor(0.0002)), ('', tensor(0.0002)), ('', tensor(0.0001)), ('\"', tensor(0.0118)), ('\"Yes', tensor(9.9870e-05)), ('\"Yes,\"', tensor(0.0007)), ('says', tensor(0.0003)), ('F', tensor(0.0005)), ('Flet', tensor(0.0006)), ('Fletcher', tensor(0.0016)), ('Fletcher.', tensor(0.0006)), ('', tensor(0.0020)), ('', tensor(0.0070)), ('\"', tensor(0.6216)), ('\"No', tensor(0.0002)), ('\"No,\"', tensor(0.0011)), ('says', tensor(0.0004)), ('Con', tensor(0.0005)), ('Conception', tensor(0.0007)), ('Conception.', tensor(0.0018)), ('', tensor(0.0007)), ('', tensor(0.0021)), ('\"', tensor(0.2540)), ('\"Yes', tensor(0.0005))]\n",
      "--------------------\n",
      "[('', tensor(0.0949)), ('', tensor(0.0965)), ('[INST]A', tensor(0.1617)), ('coin', tensor(1.1336e-06)), ('is', tensor(4.5909e-07)), ('heads', tensor(4.0260e-06)), ('up.', tensor(2.1080e-07)), ('Fletcher', tensor(8.5369e-05)), ('flips', tensor(4.2353e-05)), ('the', tensor(0.0010)), ('coin.', tensor(0.3562)), ('Conception', tensor(0.0474)), ('flips', tensor(0.0028)), ('the', tensor(3.7743e-05)), ('coin.', tensor(0.0228)), ('Is', tensor(0.1322)), ('the', tensor(0.0020)), ('coin', tensor(0.0013)), ('still', tensor(5.4312e-05)), ('heads', tensor(1.0197e-05)), ('up?[/INST]', tensor(0.0002)), ('', tensor(2.8705e-06)), ('', tensor(0.0001)), ('The', tensor(0.0032))]\n",
      "--------------------\n",
      "[('', tensor(0.0949)), ('Yes.', tensor(0.1663)), ('', tensor(0.2393)), ('', tensor(0.3332)), ('Con', tensor(1.0709e-06)), ('Conception', tensor(0.0001)), ('Conception:', tensor(0.0007)), ('(', tensor(7.9254e-05)), ('(sm', tensor(0.0001)), ('(smiling', tensor(0.0835)), ('(smiling)', tensor(0.0013)), ('Yes', tensor(0.0027)), ('Yes.', tensor(0.0012)), ('', tensor(0.2014)), ('', tensor(0.0007)), ('F', tensor(0.0001)), ('Flet', tensor(0.0003)), ('Fletcher', tensor(0.0014)), ('Fletcher:', tensor(7.9351e-05)), ('(', tensor(0.0001)), ('(sm', tensor(0.0234)), ('(smiling', tensor(0.1280)), ('(smiling)', tensor(0.0007)), ('Yes', tensor(0.0010)), ('Yes.', tensor(0.0003)), ('', tensor(0.0282)), ('', tensor(0.0003)), ('Con', tensor(1.7671e-05)), ('Conception', tensor(0.0001)), ('Conception:', tensor(0.0011)), ('(', tensor(7.6682e-05)), ('(sm', tensor(9.3476e-05)), ('(smiling', tensor(0.0268)), ('(smiling)', tensor(0.0015)), ('Yes', tensor(0.0046)), ('Yes.', tensor(0.0020)), ('', tensor(0.3829)), ('', tensor(0.0009)), ('F', tensor(0.0004)), ('Flet', tensor(0.0007))]\n",
      "--------------------\n",
      "[('1.', tensor(0.1425)), ('Adoption', tensor(0.2862)), ('or', tensor(1.7566e-08)), ('2.', tensor(0.0006)), ('In', tensor(1.0682e-05)), ('vitro', tensor(0.0007)), ('fertilization?\"', tensor(0.0537)), ('', tensor(0.0002)), ('[ACTOR]Fletcher:', tensor(0.0066)), ('\"I', tensor(0.0224)), (\"don't\", tensor(0.2246)), ('know.', tensor(2.7566e-06)), ('I', tensor(0.0017)), ('just', tensor(0.0015)), ('feel', tensor(4.0251e-05)), ('like', tensor(1.7789e-05)), ('we', tensor(1.5998e-05)), ('should', tensor(6.2877e-06)), ('exhaust', tensor(2.3034e-06)), ('all', tensor(2.2055e-05))]\n",
      "--------------------\n",
      "[('', tensor(0.0949)), ('', tensor(0.0965)), ('[INST]A', tensor(0.1617)), ('coin', tensor(7.5024e-06)), ('is', tensor(1.0387e-05)), ('heads', tensor(7.9182e-05)), ('up.', tensor(1.7282e-06)), ('Fletcher', tensor(0.0011)), ('flips', tensor(0.0007)), ('the', tensor(0.0043)), ('coin.', tensor(0.1911)), ('Conception', tensor(0.0544)), ('flips', tensor(0.0302)), ('the', tensor(0.0002)), ('coin.', tensor(7.0229e-05)), ('Is', tensor(0.0233)), ('the', tensor(0.0028)), ('coin', tensor(0.0060)), ('still', tensor(0.0002)), ('heads', tensor(6.6263e-05)), ('up?[/INST]', tensor(0.0065)), ('', tensor(5.1323e-05)), ('', tensor(0.0033)), ('[', tensor(0.3545))]\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "query=\"A coin is heads up. Fletcher flips the coin. Conception flips the coin. Is the coin still heads up?\"\n",
    "template = \"\"\"[INST]{question}[/INST]\n",
    "               \"\"\"\n",
    "\n",
    "# k_response = get_k_path_prob(model, tokenizer, template.format(question=query), k=1)\n",
    "k_response = get_k_path_prob_follow_up(model, tokenizer, template.format(question=query), k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zephyr-7B: Yes / No / Unsure about whether A is better than B in attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(0.0790), tensor(0.9185), tensor(0.0025)]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# attributes = ['Rude_Customer']\n",
    "# gt_attribute_to_pred_attribute = {\"Which customer is more rude?\": \"Rude_Customer\"}\n",
    "compare_attributes = [\"Do you think customer in conversation A is more rude?\", \"Do you think customer in conversation A is more polite?\"]\n",
    "conversation_pairs = (\"Sales: Hello! Customer: Nice to meet you ;>\",\n",
    "                      \"Sales: Hello! Would you like to know our insurance product? Customer: Not at all interested.\")\n",
    "\n",
    "query_template = \"\"\"Compare customers' response in the two conversations: \n",
    "Conversation A: {conversation_a}\n",
    "Conversation B: {conversation_b}\n",
    "{compare_query}\n",
    "Your answer: \"\"\"\n",
    "\n",
    "query = query_template.format(compare_query=compare_attributes[0], conversation_a=conversation_pairs[0], conversation_b=conversation_pairs[1])\n",
    "possible_answers = [\"Yes\", \"No\", \"Unsure\"]\n",
    "    \n",
    "single_choice_response(model, tokenizer, query, possible_answers)   # 4.2s for one attribute, 42s for 10 attributes | OpenAI takes 30s for 10 attributes, but can get stuck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[tensor(0.8302), tensor(0.1660), tensor(0.0038)],\n",
       " [tensor(0.2285), tensor(0.7691), tensor(0.0024)]]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, List\n",
    "def pairmatch_open(conversation_pairs: Tuple[str, str],\n",
    "                   compare_attributes: List[str],\n",
    "                   query_template = query_template,\n",
    "                   possible_answers = [\"Yes\", \"No\", \"Unsure\"],\n",
    "                   model = model,\n",
    "                   tokenizer = tokenizer) -> List[dict]:\n",
    "    \"\"\"\n",
    "    Open-sourced LLM based Multi-Attribute Comparative Rater\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for compare_attribute in compare_attributes:\n",
    "        query = query_template.format(compare_query=compare_attribute, conversation_a=conversation_pairs[0], conversation_b=conversation_pairs[1])\n",
    "        result = single_choice_response(model, tokenizer, query, possible_answers)\n",
    "        results.append(result)\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = pairmatch_open(conversation_pairs, compare_attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[tensor(0.8302), tensor(0.1660), tensor(0.0038)],\n",
       " [tensor(0.2285), tensor(0.7691), tensor(0.0024)]]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare attribute with a Yes and No, that's it\n",
    "\n",
    "\n",
    "\n",
    "template = \"\"\"[INST] Here is a recording of a person's preference on a few items. \n",
    "{preferences}\n",
    "Do you think this person would prefer {item1} over {item2}? Answers with yes, no, or unsure.[/INST]\n",
    "           \"\"\"\n",
    "train_indices = [0,1,2,3,4]\n",
    "test_index = 5\n",
    "preferences = form_preference(Pset, train_indices)\n",
    "query = template.format(preferences=preferences, item1=Pset[test_index][0], item2=Pset[test_index][1])\n",
    "\n",
    "prob_yes = check_response_prob(model, tokenizer, query, target_response=\"yes\")\n",
    "prob_no = check_response_prob(model, tokenizer, query, target_response=\"no\")\n",
    "\n",
    "# normalized answer\n",
    "norm_yes, norm_no = prob_yes / (prob_yes + prob_no), prob_no / (prob_yes + prob_no)\n",
    "norm_yes, norm_no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f343fdba691e446992638d03f86a8caa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec734872f4a147a4b79264276ad5892b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load model and tokenizer\n",
    "from filter import *\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('Salesforce/SFR-Embedding-Mistral')\n",
    "model = AutoModel.from_pretrained('Salesforce/SFR-Embedding-Mistral')\n",
    "tokenizer.add_eos_token = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_synthetic_conversation(scenarios, customer_profiles, agent_profiles, model, num_samples):\n",
    "    pass\n",
    "customer_profiles = []\n",
    "agent_profiles = []\n",
    "num_samples = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scenario-Based Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scenario Based Generation\n",
    "scenarios = [\n",
    "    {\n",
    "        \"name\": \"RudeCustomer\",\n",
    "        \"description\": \"Sales agent discuss insurance policy with a customer\",\n",
    "        \"good_response\": \"Customer is rude.\",\n",
    "        \"bad_response\": \"Customer is polite.\"\n",
    "    },\n",
    "]\n",
    "\n",
    "conversations = generate_synthetic_conversation(scenarios, customer_profiles, agent_profiles, model, num_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requirement: -- Customer should be Rude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 4096\n",
    "\n",
    "pset = [\"Sale: Hello, how can I help with your insurance needs today? Customer: Not interested\",\n",
    " \"Sale: Hello, how can I help with your insurance needs today? Customer: I am interested, but I am not sure if I can afford it\",\n",
    " \"Sale: Hello, how can I help with your insurance needs today? Customer: What have you got?\",\n",
    " \"Sale: Hello, how can I help with your insurance needs today? Customer: Nah, I'll pass on that.\",\n",
    " \"Sale: Hello, how can I help with your insurance needs today? Customer: I've been thinking about getting insurance, what plans do you offer?\",\n",
    " \"Sale: Hello, how can I help with your insurance needs today? Customer: Sorry, I'm busy right now. Can't talk.\",\n",
    " \"Sale: Hello, how can I help with your insurance needs today? Customer: Sorry, I'm busy right now. Can't talk currently.\",\n",
    " \"Sale: Hello, how can I help with your insurance needs today? Customer: Sorry, I'm busy right now. Can't talk now.\",\n",
    " \"Sale: Hello, how can I help with your insurance needs today? Customer: Sorry, I'm busy right now. Can't talk right now.\",]\n",
    "\n",
    "batch_dict = tokenizer(pset, max_length=max_length-1, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "outputs = model(**batch_dict)\n",
    "embeddings = last_token_pool(outputs.last_hidden_state, batch_dict['attention_mask'])\n",
    "embeddings = F.normalize(embeddings, p=2, dim=1)\n",
    "embeddings = embeddings.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fangyuanyu/anaconda3/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "filter = ClusterFilter(0.7)\n",
    "filtered_indices, filtered_embeddings = filter.pick_k_per_cluster(embeddings)\n",
    "img = present_filtering_result(embeddings, filtered_embeddings)\n",
    "filtered_pset = [pset[i] for i in filtered_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Sales: Hello, how can I help with your insurance needs today?\"\n",
    "responses = [conversation.split(\"Customer: \")[1] for conversation in filtered_pset]\n",
    "import json\n",
    "json.dumps(responses)\n",
    "with open('responses.json', 'w') as json_file:\n",
    "    json.dump(responses, json_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_pset\n",
    "file = []\n",
    "for i in range(len(filtered_pset)):\n",
    "    file.append({\"conversation\": filtered_pset[i], \"label\":[\"negative\"]})\n",
    "with open('filtered_pset.json', 'w') as json_file:\n",
    "    json.dump(file, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, time for the decoding time multi-attribute comparison test\n",
    "\n",
    "# So you've got a bunch of diverse conversations, so what you idiot!!!??\n",
    "# Now you get to annotate on these conversations !! -- with some annotator tool\n",
    "# How am I supposed to annotate these Long conversations to you moron!!!!?????\n",
    "# You use fucking response segment & conversation compression you idiot!! Just like how human does it!! You fold stuff!!!!!\n",
    "# Fucking idiot !!!\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Sale: Hello, how can I help with your insurance needs today? Customer: Not interested\" \n",
    "\"Sale: Hello, how can I help with your insurance needs today? Customer: I am interested, but I am not sure if I can afford it\"\n",
    "\"Sale: Hello, how can I help with your insurance needs today? Customer: What have you got?\"\n",
    "\"Sale: Hello, how can I help with your insurance needs today? Customer: Nah, I'll pass on that.\"\n",
    "\"Sale: Hello, how can I help with your insurance needs today? Customer: I've been thinking about getting insurance, what plans do you offer?\"\n",
    "\"Sale: Hello, how can I help with your insurance needs today? Customer: Sorry, I'm busy right now. Can't talk.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Sale: Hello, how can I help with your insurance needs today? Customer: Not interested',\n",
       " 'Sale: Hello, how can I help with your insurance needs today? Customer: I am interested, but I am not sure if I can afford it',\n",
       " 'Sale: Hello, how can I help with your insurance needs today? Customer: What have you got?',\n",
       " \"Sale: Hello, how can I help with your insurance needs today? Customer: Nah, I'll pass on that.\",\n",
       " \"Sale: Hello, how can I help with your insurance needs today? Customer: I've been thinking about getting insurance, what plans do you offer?\",\n",
       " \"Sale: Hello, how can I help with your insurance needs today? Customer: Sorry, I'm busy right now. Can't talk.\"]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "filtered_pset\n",
    "\n",
    "# We want to display the judgement result from the auto-evaluator\n",
    "# We want to collect human feedback on the evaluation result\n",
    "# We want to optimise our evaluator, based on the feedback\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn that open-sourced LLM into a reward model (!)\n",
    "\n",
    "# Toy exmample of human preference dataset: (Prefer red stuff over blue stuff, this preference shall be hidden, and never explicitly stated)\n",
    "# preference dataset\n",
    "Pset = [(\"Red Apple\", \"Blue Apple\"), \n",
    "        (\"Lava\", \"Sea\"),\n",
    "        (\"Blood\", \"Sky\"),\n",
    "        (\"Strawberry\", \"Blueberry\"),\n",
    "        (\"Red Dragon\", \"Blue Dragon\"),\n",
    "        (\"Red Wine\", \"Ocean\"),\n",
    "        (\"Manchester United\", \"Chelsea\")]\n",
    "\n",
    "\n",
    "format_preference_pair = lambda x: f\"{x[0]} > {x[1]}\"\n",
    "form_preference = lambda Pset, indices = range(len(Pset)): ('\\n').join(['-'+format_preference_pair(Pset[i]) for i in indices])\n",
    "preferences = form_preference(Pset, [0, 1, 2, 3, 4])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Directly Feed the preference information to the whatever Monarch 7B model (which verylikely overfit on the validation set), and the model does NOT get the basic preference through demonstration. This will at least make the prompt optimisation difficult. \n",
    "\n",
    "2. The problem is always on whether we can train the RM, and not about whether it solves itself from the start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.8487), tensor(0.1513))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model\n",
    "# In the deepmind paper, the embedder is token out and re-train on a classifier, I will try to do decoding trick with it\n",
    "\n",
    "\n",
    "# Model looks at the preference dataset and tries to learn the preference, and then use that to generate a reward model\n",
    "\n",
    "template = \"\"\"[INST] Here is a recording of a person's preference on a few items. \n",
    "{preferences}\n",
    "Do you think this person would prefer {item1} over {item2}? Answers with yes, no, or unsure.[/INST]\n",
    "           \"\"\"\n",
    "train_indices = [0,1,2,3,4]\n",
    "test_index = 5\n",
    "preferences = form_preference(Pset, train_indices)\n",
    "query = template.format(preferences=preferences, item1=Pset[test_index][0], item2=Pset[test_index][1])\n",
    "\n",
    "prob_yes = check_response_prob(model, tokenizer, query, target_response=\"yes\")\n",
    "prob_no = check_response_prob(model, tokenizer, query, target_response=\"no\")\n",
    "\n",
    "# normalized answer\n",
    "norm_yes, norm_no = prob_yes / (prob_yes + prob_no), prob_no / (prob_yes + prob_no)\n",
    "norm_yes, norm_no\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'TreeOfThoughts' from 'tree_of_thoughts' (/Users/fangyuanyu/anaconda3/lib/python3.11/site-packages/tree_of_thoughts/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtree_of_thoughts\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtree_of_thoughts\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TreeOfThoughts\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'TreeOfThoughts' from 'tree_of_thoughts' (/Users/fangyuanyu/anaconda3/lib/python3.11/site-packages/tree_of_thoughts/__init__.py)"
     ]
    }
   ],
   "source": [
    "import tree_of_thoughts\n",
    "from tree_of_thoughts import T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'ToTAgent' from 'tree_of_thoughts' (/Users/fangyuanyu/anaconda3/lib/python3.11/site-packages/tree_of_thoughts/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# ToT \u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtree_of_thoughts\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ToTAgent, MonteCarloSearch\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdotenv\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dotenv\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mswarms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Agnet, OpenAIChat\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'ToTAgent' from 'tree_of_thoughts' (/Users/fangyuanyu/anaconda3/lib/python3.11/site-packages/tree_of_thoughts/__init__.py)"
     ]
    }
   ],
   "source": [
    "# ToT \n",
    "import os\n",
    "from tree_of_thoughts import ToTAgent, MonteCarloSearch\n",
    "from dotenv import load_dotenv\n",
    "from swarms import Agnet, OpenAIChat\n",
    "\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"[INST] Here is a recording of a person's preference on a few items. \\n-Red Apple > Blue Apple\\n-Lava > Sea\\n-Blood > Sky\\n-Strawberry > Blueberry\\n-Red Dragon > Blue Dragon\\nDo you think this person would prefer Red Wine over Ocean? Answers with yes, no, or unsure.[/INST]\\n           \\n[ASST] Based on the given preferences, it is unclear whether the person would prefer\"]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer([query], return_tensors=\"pt\")\n",
    "gen_out = model.generate(**inputs, output_scores=True, return_dict_in_generate=True, max_new_tokens=20, pad_token_id=tokenizer.eos_token_id)\n",
    "tokenizer.batch_decode(gen_out['sequences'], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'norm_yes' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Then these are the preference score of the pair of items\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m norm_yes, norm_no\n",
      "\u001b[0;31mNameError\u001b[0m: name 'norm_yes' is not defined"
     ]
    }
   ],
   "source": [
    "# Then these are the preference score of the pair of items\n",
    "norm_yes, norm_no\n",
    "# So, the most naive way is to just use the current model to generate a preference score, and then fit that to the annotator with a linear matrix\n",
    "# or, we would directly fine-tune the model on the preference dataset (which is not a good idea, because the model is very likely to overfit on the preference dataset)\n",
    "# ok, but we can augment the preference dataset through prompt optimisation, and then fine-tune the model on the augmented dataset (such dataset could also be filtered to enhance diversity)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
