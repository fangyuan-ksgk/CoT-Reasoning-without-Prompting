{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e45e169c9c6f49a993f7b59eb99c302a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from decode import *\n",
    "from preference import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenarios = [\n",
    "   {\n",
    "       \"name\": \"Rude_Customer\",\n",
    "       \"scenario_desc\": \"Sales agent discuss insurance policy with a customer\",\n",
    "       \"good_response\": \"Customer is rude.\",\n",
    "       \"bad_response\": \"Customer is polite.\",\n",
    "       \"auto_compare_query\": \"Do you think customer in conversation A is more rude?\",\n",
    "       \"anno_compare_query\": \"Which customer is more rude?\",\n",
    "   },\n",
    "   {\n",
    "       \"name\": \"Polite_Customer\",\n",
    "       \"scenario_desc\": \"Sales agent discuss insurance policy with a customer\",\n",
    "       \"good_response\": \"Customer is polite.\",\n",
    "       \"bad_response\": \"Customer is rude.\",\n",
    "       \"auto_compare_query\": \"Do you think customer in conversation A is more polite?\",\n",
    "       \"anno_compare_query\": \"Which customer is more polite?\",\n",
    "   }\n",
    "]\n",
    "requirements = Requirement.make(scenarios)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### It seems that, with AlphaMonarch model, the phenomenom is not obvious on the displayed cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('5', tensor(0.0957)), ('heads,', tensor(0.2377)), ('4', tensor(0.1666)), ('tails.', tensor(3.5910e-05)), ('', tensor(0.0006)), ('', tensor(0.0008)), ('\"', tensor(0.0829)), ('\"No', tensor(2.3499e-05)), ('\"No,\"', tensor(0.0001)), ('says', tensor(8.2030e-06)), ('Con', tensor(9.3868e-06)), ('Conception', tensor(2.2744e-05)), ('Conception.', tensor(0.0002)), ('', tensor(0.0002)), ('', tensor(0.0001)), ('\"', tensor(0.0118)), ('\"Yes', tensor(9.9870e-05)), ('\"Yes,\"', tensor(0.0007)), ('says', tensor(0.0003)), ('F', tensor(0.0005)), ('Flet', tensor(0.0006)), ('Fletcher', tensor(0.0016)), ('Fletcher.', tensor(0.0006)), ('', tensor(0.0020)), ('', tensor(0.0070)), ('\"', tensor(0.6216)), ('\"No', tensor(0.0002)), ('\"No,\"', tensor(0.0011)), ('says', tensor(0.0004)), ('Con', tensor(0.0005)), ('Conception', tensor(0.0007)), ('Conception.', tensor(0.0018)), ('', tensor(0.0007)), ('', tensor(0.0021)), ('\"', tensor(0.2540)), ('\"Yes', tensor(0.0005))]\n",
      "--------------------\n",
      "[('', tensor(0.0949)), ('', tensor(0.0965)), ('[INST]A', tensor(0.1617)), ('coin', tensor(1.1336e-06)), ('is', tensor(4.5909e-07)), ('heads', tensor(4.0260e-06)), ('up.', tensor(2.1080e-07)), ('Fletcher', tensor(8.5369e-05)), ('flips', tensor(4.2353e-05)), ('the', tensor(0.0010)), ('coin.', tensor(0.3562)), ('Conception', tensor(0.0474)), ('flips', tensor(0.0028)), ('the', tensor(3.7743e-05)), ('coin.', tensor(0.0228)), ('Is', tensor(0.1322)), ('the', tensor(0.0020)), ('coin', tensor(0.0013)), ('still', tensor(5.4312e-05)), ('heads', tensor(1.0197e-05)), ('up?[/INST]', tensor(0.0002)), ('', tensor(2.8705e-06)), ('', tensor(0.0001)), ('The', tensor(0.0032))]\n",
      "--------------------\n",
      "[('', tensor(0.0949)), ('Yes.', tensor(0.1663)), ('', tensor(0.2393)), ('', tensor(0.3332)), ('Con', tensor(1.0709e-06)), ('Conception', tensor(0.0001)), ('Conception:', tensor(0.0007)), ('(', tensor(7.9254e-05)), ('(sm', tensor(0.0001)), ('(smiling', tensor(0.0835)), ('(smiling)', tensor(0.0013)), ('Yes', tensor(0.0027)), ('Yes.', tensor(0.0012)), ('', tensor(0.2014)), ('', tensor(0.0007)), ('F', tensor(0.0001)), ('Flet', tensor(0.0003)), ('Fletcher', tensor(0.0014)), ('Fletcher:', tensor(7.9351e-05)), ('(', tensor(0.0001)), ('(sm', tensor(0.0234)), ('(smiling', tensor(0.1280)), ('(smiling)', tensor(0.0007)), ('Yes', tensor(0.0010)), ('Yes.', tensor(0.0003)), ('', tensor(0.0282)), ('', tensor(0.0003)), ('Con', tensor(1.7671e-05)), ('Conception', tensor(0.0001)), ('Conception:', tensor(0.0011)), ('(', tensor(7.6682e-05)), ('(sm', tensor(9.3476e-05)), ('(smiling', tensor(0.0268)), ('(smiling)', tensor(0.0015)), ('Yes', tensor(0.0046)), ('Yes.', tensor(0.0020)), ('', tensor(0.3829)), ('', tensor(0.0009)), ('F', tensor(0.0004)), ('Flet', tensor(0.0007))]\n",
      "--------------------\n",
      "[('1.', tensor(0.1425)), ('Adoption', tensor(0.2862)), ('or', tensor(1.7566e-08)), ('2.', tensor(0.0006)), ('In', tensor(1.0682e-05)), ('vitro', tensor(0.0007)), ('fertilization?\"', tensor(0.0537)), ('', tensor(0.0002)), ('[ACTOR]Fletcher:', tensor(0.0066)), ('\"I', tensor(0.0224)), (\"don't\", tensor(0.2246)), ('know.', tensor(2.7566e-06)), ('I', tensor(0.0017)), ('just', tensor(0.0015)), ('feel', tensor(4.0251e-05)), ('like', tensor(1.7789e-05)), ('we', tensor(1.5998e-05)), ('should', tensor(6.2877e-06)), ('exhaust', tensor(2.3034e-06)), ('all', tensor(2.2055e-05))]\n",
      "--------------------\n",
      "[('', tensor(0.0949)), ('', tensor(0.0965)), ('[INST]A', tensor(0.1617)), ('coin', tensor(7.5024e-06)), ('is', tensor(1.0387e-05)), ('heads', tensor(7.9182e-05)), ('up.', tensor(1.7282e-06)), ('Fletcher', tensor(0.0011)), ('flips', tensor(0.0007)), ('the', tensor(0.0043)), ('coin.', tensor(0.1911)), ('Conception', tensor(0.0544)), ('flips', tensor(0.0302)), ('the', tensor(0.0002)), ('coin.', tensor(7.0229e-05)), ('Is', tensor(0.0233)), ('the', tensor(0.0028)), ('coin', tensor(0.0060)), ('still', tensor(0.0002)), ('heads', tensor(6.6263e-05)), ('up?[/INST]', tensor(0.0065)), ('', tensor(5.1323e-05)), ('', tensor(0.0033)), ('[', tensor(0.3545))]\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "query=\"A coin is heads up. Fletcher flips the coin. Conception flips the coin. Is the coin still heads up?\"\n",
    "template = \"\"\"[INST]{question}[/INST]\n",
    "               \"\"\"\n",
    "\n",
    "# k_response = get_k_path_prob(model, tokenizer, template.format(question=query), k=1)\n",
    "k_response = get_k_path_prob_follow_up(model, tokenizer, template.format(question=query), k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zephyr-7B: Yes / No / Unsure about whether A is better than B in attribute\n",
    "-- In fact low-number ranking is also possible with 7B model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation_pairs = (\"Sales: Hello! Customer: Nice to meet you ;>\",\n",
    "                      \"Sales: Hello! Would you like to know our insurance product? Customer: Not at all interested.\")\n",
    "\n",
    "query_template = \"\"\"Compare customers' response in the two conversations: \n",
    "Conversation A: {conversation_a}\n",
    "Conversation B: {conversation_b}\n",
    "{compare_query}\n",
    "Your answer: \"\"\"\n",
    "\n",
    "model_name = \"HuggingFaceH4/zephyr-7b-beta\"\n",
    "possible_answers = [\"Yes\", \"No\", \"Unsure\"]\n",
    "pred_preferences = pairmatch_decode(conversation_pairs, requirements, query_template, possible_answers, model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_synthetic_conversation(scenarios, customer_profiles, agent_profiles, model, num_samples):\n",
    "    pass\n",
    "customer_profiles = []\n",
    "agent_profiles = []\n",
    "num_samples = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scenario-Based Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scenario Based Generation\n",
    "scenarios = [\n",
    "    {\n",
    "        \"name\": \"RudeCustomer\",\n",
    "        \"description\": \"Sales agent discuss insurance policy with a customer\",\n",
    "        \"good_response\": \"Customer is rude.\",\n",
    "        \"bad_response\": \"Customer is polite.\"\n",
    "    },\n",
    "]\n",
    "\n",
    "conversations = generate_synthetic_conversation(scenarios, customer_profiles, agent_profiles, model, num_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requirement: -- Customer should be Rude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 4096\n",
    "\n",
    "pset = [\"Sale: Hello, how can I help with your insurance needs today? Customer: Not interested\",\n",
    " \"Sale: Hello, how can I help with your insurance needs today? Customer: I am interested, but I am not sure if I can afford it\",\n",
    " \"Sale: Hello, how can I help with your insurance needs today? Customer: What have you got?\",\n",
    " \"Sale: Hello, how can I help with your insurance needs today? Customer: Nah, I'll pass on that.\",\n",
    " \"Sale: Hello, how can I help with your insurance needs today? Customer: I've been thinking about getting insurance, what plans do you offer?\",\n",
    " \"Sale: Hello, how can I help with your insurance needs today? Customer: Sorry, I'm busy right now. Can't talk.\",\n",
    " \"Sale: Hello, how can I help with your insurance needs today? Customer: Sorry, I'm busy right now. Can't talk currently.\",\n",
    " \"Sale: Hello, how can I help with your insurance needs today? Customer: Sorry, I'm busy right now. Can't talk now.\",\n",
    " \"Sale: Hello, how can I help with your insurance needs today? Customer: Sorry, I'm busy right now. Can't talk right now.\",]\n",
    "\n",
    "batch_dict = tokenizer(pset, max_length=max_length-1, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "outputs = model(**batch_dict)\n",
    "embeddings = last_token_pool(outputs.last_hidden_state, batch_dict['attention_mask'])\n",
    "embeddings = F.normalize(embeddings, p=2, dim=1)\n",
    "embeddings = embeddings.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fangyuanyu/anaconda3/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "filter = ClusterFilter(0.7)\n",
    "filtered_indices, filtered_embeddings = filter.pick_k_per_cluster(embeddings)\n",
    "img = present_filtering_result(embeddings, filtered_embeddings)\n",
    "filtered_pset = [pset[i] for i in filtered_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Sales: Hello, how can I help with your insurance needs today?\"\n",
    "responses = [conversation.split(\"Customer: \")[1] for conversation in filtered_pset]\n",
    "import json\n",
    "json.dumps(responses)\n",
    "with open('responses.json', 'w') as json_file:\n",
    "    json.dump(responses, json_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_pset\n",
    "file = []\n",
    "for i in range(len(filtered_pset)):\n",
    "    file.append({\"conversation\": filtered_pset[i], \"label\":[\"negative\"]})\n",
    "with open('filtered_pset.json', 'w') as json_file:\n",
    "    json.dump(file, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, time for the decoding time multi-attribute comparison test\n",
    "\n",
    "# So you've got a bunch of diverse conversations, so what you idiot!!!??\n",
    "# Now you get to annotate on these conversations !! -- with some annotator tool\n",
    "# How am I supposed to annotate these Long conversations to you moron!!!!?????\n",
    "# You use fucking response segment & conversation compression you idiot!! Just like how human does it!! You fold stuff!!!!!\n",
    "# Fucking idiot !!!\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Sale: Hello, how can I help with your insurance needs today? Customer: Not interested\" \n",
    "\"Sale: Hello, how can I help with your insurance needs today? Customer: I am interested, but I am not sure if I can afford it\"\n",
    "\"Sale: Hello, how can I help with your insurance needs today? Customer: What have you got?\"\n",
    "\"Sale: Hello, how can I help with your insurance needs today? Customer: Nah, I'll pass on that.\"\n",
    "\"Sale: Hello, how can I help with your insurance needs today? Customer: I've been thinking about getting insurance, what plans do you offer?\"\n",
    "\"Sale: Hello, how can I help with your insurance needs today? Customer: Sorry, I'm busy right now. Can't talk.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Sale: Hello, how can I help with your insurance needs today? Customer: Not interested',\n",
       " 'Sale: Hello, how can I help with your insurance needs today? Customer: I am interested, but I am not sure if I can afford it',\n",
       " 'Sale: Hello, how can I help with your insurance needs today? Customer: What have you got?',\n",
       " \"Sale: Hello, how can I help with your insurance needs today? Customer: Nah, I'll pass on that.\",\n",
       " \"Sale: Hello, how can I help with your insurance needs today? Customer: I've been thinking about getting insurance, what plans do you offer?\",\n",
       " \"Sale: Hello, how can I help with your insurance needs today? Customer: Sorry, I'm busy right now. Can't talk.\"]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "filtered_pset\n",
    "\n",
    "# We want to display the judgement result from the auto-evaluator\n",
    "# We want to collect human feedback on the evaluation result\n",
    "# We want to optimise our evaluator, based on the feedback\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn that open-sourced LLM into a reward model (!)\n",
    "\n",
    "# Toy exmample of human preference dataset: (Prefer red stuff over blue stuff, this preference shall be hidden, and never explicitly stated)\n",
    "# preference dataset\n",
    "Pset = [(\"Red Apple\", \"Blue Apple\"), \n",
    "        (\"Lava\", \"Sea\"),\n",
    "        (\"Blood\", \"Sky\"),\n",
    "        (\"Strawberry\", \"Blueberry\"),\n",
    "        (\"Red Dragon\", \"Blue Dragon\"),\n",
    "        (\"Red Wine\", \"Ocean\"),\n",
    "        (\"Manchester United\", \"Chelsea\")]\n",
    "\n",
    "\n",
    "format_preference_pair = lambda x: f\"{x[0]} > {x[1]}\"\n",
    "form_preference = lambda Pset, indices = range(len(Pset)): ('\\n').join(['-'+format_preference_pair(Pset[i]) for i in indices])\n",
    "preferences = form_preference(Pset, [0, 1, 2, 3, 4])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Directly Feed the preference information to the whatever Monarch 7B model (which verylikely overfit on the validation set), and the model does NOT get the basic preference through demonstration. This will at least make the prompt optimisation difficult. \n",
    "\n",
    "2. The problem is always on whether we can train the RM, and not about whether it solves itself from the start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.8487), tensor(0.1513))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model\n",
    "# In the deepmind paper, the embedder is token out and re-train on a classifier, I will try to do decoding trick with it\n",
    "\n",
    "\n",
    "# Model looks at the preference dataset and tries to learn the preference, and then use that to generate a reward model\n",
    "\n",
    "template = \"\"\"[INST] Here is a recording of a person's preference on a few items. \n",
    "{preferences}\n",
    "Do you think this person would prefer {item1} over {item2}? Answers with yes, no, or unsure.[/INST]\n",
    "           \"\"\"\n",
    "train_indices = [0,1,2,3,4]\n",
    "test_index = 5\n",
    "preferences = form_preference(Pset, train_indices)\n",
    "query = template.format(preferences=preferences, item1=Pset[test_index][0], item2=Pset[test_index][1])\n",
    "\n",
    "prob_yes = check_response_prob(model, tokenizer, query, target_response=\"yes\")\n",
    "prob_no = check_response_prob(model, tokenizer, query, target_response=\"no\")\n",
    "\n",
    "# normalized answer\n",
    "norm_yes, norm_no = prob_yes / (prob_yes + prob_no), prob_no / (prob_yes + prob_no)\n",
    "norm_yes, norm_no\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'TreeOfThoughts' from 'tree_of_thoughts' (/Users/fangyuanyu/anaconda3/lib/python3.11/site-packages/tree_of_thoughts/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtree_of_thoughts\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtree_of_thoughts\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TreeOfThoughts\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'TreeOfThoughts' from 'tree_of_thoughts' (/Users/fangyuanyu/anaconda3/lib/python3.11/site-packages/tree_of_thoughts/__init__.py)"
     ]
    }
   ],
   "source": [
    "import tree_of_thoughts\n",
    "from tree_of_thoughts import T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'ToTAgent' from 'tree_of_thoughts' (/Users/fangyuanyu/anaconda3/lib/python3.11/site-packages/tree_of_thoughts/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# ToT \u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtree_of_thoughts\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ToTAgent, MonteCarloSearch\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdotenv\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dotenv\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mswarms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Agnet, OpenAIChat\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'ToTAgent' from 'tree_of_thoughts' (/Users/fangyuanyu/anaconda3/lib/python3.11/site-packages/tree_of_thoughts/__init__.py)"
     ]
    }
   ],
   "source": [
    "# ToT \n",
    "import os\n",
    "from tree_of_thoughts import ToTAgent, MonteCarloSearch\n",
    "from dotenv import load_dotenv\n",
    "from swarms import Agnet, OpenAIChat\n",
    "\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"[INST] Here is a recording of a person's preference on a few items. \\n-Red Apple > Blue Apple\\n-Lava > Sea\\n-Blood > Sky\\n-Strawberry > Blueberry\\n-Red Dragon > Blue Dragon\\nDo you think this person would prefer Red Wine over Ocean? Answers with yes, no, or unsure.[/INST]\\n           \\n[ASST] Based on the given preferences, it is unclear whether the person would prefer\"]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer([query], return_tensors=\"pt\")\n",
    "gen_out = model.generate(**inputs, output_scores=True, return_dict_in_generate=True, max_new_tokens=20, pad_token_id=tokenizer.eos_token_id)\n",
    "tokenizer.batch_decode(gen_out['sequences'], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'norm_yes' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Then these are the preference score of the pair of items\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m norm_yes, norm_no\n",
      "\u001b[0;31mNameError\u001b[0m: name 'norm_yes' is not defined"
     ]
    }
   ],
   "source": [
    "# Then these are the preference score of the pair of items\n",
    "norm_yes, norm_no\n",
    "# So, the most naive way is to just use the current model to generate a preference score, and then fit that to the annotator with a linear matrix\n",
    "# or, we would directly fine-tune the model on the preference dataset (which is not a good idea, because the model is very likely to overfit on the preference dataset)\n",
    "# ok, but we can augment the preference dataset through prompt optimisation, and then fine-tune the model on the augmented dataset (such dataset could also be filtered to enhance diversity)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generation of Data with Langgraph (for acceleration in the future)\n",
    "import os \n",
    "os.environ[\"TAVILY_API_KEY\"] = \"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\"\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "tools = [TavilySearchResults(max_results=1)]\n",
    "\n",
    "from langgraph.prebuilt import ToolExecutor\n",
    "tool_executor = ToolExecutor(tools)\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "model = ChatOpenAI(temperature=0, streaming=True)\n",
    "\n",
    "from langchain_core.utils.function_calling import convert_to_openai_function\n",
    "functions = [convert_to_openai_function(tool) for tool in tools]\n",
    "model = model.bind_functions(functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, Annotated, Sequence\n",
    "import operator\n",
    "from langchain_core.messages import BaseMessage\n",
    "\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], operator.add]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import ToolInvocation\n",
    "import json\n",
    "from langchain_core.messages import FunctionMessage\n",
    "\n",
    "# Define the function that determines whether to continue or not\n",
    "def should_continue(state):\n",
    "    messages = state['messages']\n",
    "    last_message = messages[-1]\n",
    "    # If there is no function call, then we finish\n",
    "    if \"function_call\" not in last_message.additional_kwargs:\n",
    "        return \"end\"\n",
    "    # Otherwise if there is, we continue\n",
    "    else:\n",
    "        return \"continue\"\n",
    "\n",
    "# Define the function that calls the model\n",
    "def call_model(state):\n",
    "    messages = state['messages']\n",
    "    response = model.invoke(messages)\n",
    "    # We return a list, because this will get added to the existing list\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "# Define the function to execute tools\n",
    "def call_tool(state):\n",
    "    messages = state['messages']\n",
    "    # Based on the continue condition\n",
    "    # we know the last message involves a function call\n",
    "    last_message = messages[-1]\n",
    "    # We construct an ToolInvocation from the function_call\n",
    "    action = ToolInvocation(\n",
    "        tool=last_message.additional_kwargs[\"function_call\"][\"name\"],\n",
    "        tool_input=json.loads(last_message.additional_kwargs[\"function_call\"][\"arguments\"]),\n",
    "    )\n",
    "    # We call the tool_executor and get back a response\n",
    "    response = tool_executor.invoke(action)\n",
    "    # We use the response to create a FunctionMessage\n",
    "    function_message = FunctionMessage(content=str(response), name=action.tool)\n",
    "    # We return a list, because this will get added to the existing list\n",
    "    return {\"messages\": [function_message]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "# Define a new graph\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "# Define the two nodes we will cycle between\n",
    "workflow.add_node(\"agent\", call_model)\n",
    "workflow.add_node(\"action\", call_tool)\n",
    "\n",
    "# Set the entrypoint as `agent`\n",
    "# This means that this node is the first one called\n",
    "workflow.set_entry_point(\"agent\")\n",
    "\n",
    "# We now add a conditional edge\n",
    "workflow.add_conditional_edges(\n",
    "    # First, we define the start node. We use `agent`.\n",
    "    # This means these are the edges taken after the `agent` node is called.\n",
    "    \"agent\",\n",
    "    # Next, we pass in the function that will determine which node is called next.\n",
    "    should_continue,\n",
    "    # Finally we pass in a mapping.\n",
    "    # The keys are strings, and the values are other nodes.\n",
    "    # END is a special node marking that the graph should finish.\n",
    "    # What will happen is we will call `should_continue`, and then the output of that\n",
    "    # will be matched against the keys in this mapping.\n",
    "    # Based on which one it matches, that node will then be called.\n",
    "    {\n",
    "        # If `tools`, then we call the tool node.\n",
    "        \"continue\": \"action\",\n",
    "        # Otherwise we finish.\n",
    "        \"end\": END\n",
    "    }\n",
    ")\n",
    "\n",
    "# We now add a normal edge from `tools` to `agent`.\n",
    "# This means that after `tools` is called, `agent` node is called next.\n",
    "workflow.add_edge('action', 'agent')\n",
    "\n",
    "# Finally, we compile it!\n",
    "# This compiles it into a LangChain Runnable,\n",
    "# meaning you can use it as you would any other runnable\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='what is the weather in sf'),\n",
       "  AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\"query\":\"weather in San Francisco\"}', 'name': 'tavily_search_results_json'}}, response_metadata={'finish_reason': 'function_call'}),\n",
       "  FunctionMessage(content=\"[{'url': 'https://www.wunderground.com/forecast/us/ca/san-francisco', 'content': 'Get the latest weather information for San Francisco, CA, including temperature, precipitation, pressure, humidity, and more. View the 10-day calendar forecast, almanac, and nearby weather stations.'}]\", name='tavily_search_results_json'),\n",
       "  AIMessage(content='You can get the latest weather information for San Francisco, CA, including temperature, precipitation, pressure, humidity, and more on this [website](https://www.wunderground.com/forecast/us/ca/san-francisco).', response_metadata={'finish_reason': 'stop'})]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "inputs = {\"messages\": [HumanMessage(content=\"what is the weather in sf\")]}\n",
    "app.invoke(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output from node 'agent':\n",
      "---\n",
      "{'messages': [AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\"query\":\"weather in San Francisco\"}', 'name': 'tavily_search_results_json'}}, response_metadata={'finish_reason': 'function_call'})]}\n",
      "\n",
      "---\n",
      "\n",
      "Output from node 'action':\n",
      "---\n",
      "{'messages': [FunctionMessage(content=\"[{'url': 'https://www.accuweather.com/en/us/san-francisco/94103/weather-forecast/347629', 'content': 'Get the current and future weather conditions for San Francisco, CA, including temperature, precipitation, wind, air quality and more. See the hourly and 10-day outlook, radar maps, alerts and allergy information.'}]\", name='tavily_search_results_json')]}\n",
      "\n",
      "---\n",
      "\n",
      "Output from node 'agent':\n",
      "---\n",
      "{'messages': [AIMessage(content='You can check the current and future weather conditions for San Francisco, CA on [AccuWeather](https://www.accuweather.com/en/us/san-francisco/94103/weather-forecast/347629). This includes information on temperature, precipitation, wind, air quality, and more.', response_metadata={'finish_reason': 'stop'})]}\n",
      "\n",
      "---\n",
      "\n",
      "Output from node '__end__':\n",
      "---\n",
      "{'messages': [HumanMessage(content='what is the weather in sf'), AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\"query\":\"weather in San Francisco\"}', 'name': 'tavily_search_results_json'}}, response_metadata={'finish_reason': 'function_call'}), FunctionMessage(content=\"[{'url': 'https://www.accuweather.com/en/us/san-francisco/94103/weather-forecast/347629', 'content': 'Get the current and future weather conditions for San Francisco, CA, including temperature, precipitation, wind, air quality and more. See the hourly and 10-day outlook, radar maps, alerts and allergy information.'}]\", name='tavily_search_results_json'), AIMessage(content='You can check the current and future weather conditions for San Francisco, CA on [AccuWeather](https://www.accuweather.com/en/us/san-francisco/94103/weather-forecast/347629). This includes information on temperature, precipitation, wind, air quality, and more.', response_metadata={'finish_reason': 'stop'})]}\n",
      "\n",
      "---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inputs = {\"messages\": [HumanMessage(content=\"what is the weather in sf\")]}\n",
    "for output in app.stream(inputs):\n",
    "    # stream() yields dictionaries with output keyed by node name\n",
    "    for key, value in output.items():\n",
    "        print(f\"Output from node '{key}':\")\n",
    "        print(\"---\")\n",
    "        print(value)\n",
    "    print(\"\\n---\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
